{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyarrow matplotlib sentencepiece pandas\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33700, 6)\n",
      "0        [Two young guys with shaggy hair look at their...\n",
      "1        [Several men in hard hats are operating a gian...\n",
      "2        [A child in a pink dress is climbing up a set ...\n",
      "3        [Someone in a blue shirt and hat is standing o...\n",
      "4        [Two men, one in a gray shirt, one in a black ...\n",
      "                               ...                        \n",
      "33695    [A baby is hold an adults fingers on the beach...\n",
      "33696    [Two men wearing hats play accordions while st...\n",
      "33697    [A boy with whipped cream on his face after sm...\n",
      "33698    [A boy in a red and white shirt kicks a large ...\n",
      "33699    [One person holds a brown and white puppy whil...\n",
      "Name: caption, Length: 33700, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../dataset/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_full_text_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../dataset/corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 168500 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=10508595\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9516% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=51\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999516\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 168500 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=9623698\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 26882 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 168500\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 16421\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 16421 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14451 obj=8.97085 num_tokens=32229 num_tokens/piece=2.23023\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10019 obj=6.95145 num_tokens=32079 num_tokens/piece=3.20182\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_full_text_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_full_text_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Read data from both Parquet files\n",
    "train_0 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_1 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_2 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_3 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_4 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_5 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_6 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_7 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_8 = pd.read_parquet('../dataset/0001.parquet')\n",
    "\n",
    "train = pd.concat([train_0, train_1, train_2, train_3, train_4, train_5, train_6, train_7, train_8,], ignore_index=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# # Concatenate elements within each row\n",
    "# train['caption'] = train['caption'].apply('\\n'.join)\n",
    "# # Concatenate all rows into one long text string\n",
    "# long_text = train['caption'].str.cat(sep='\\n')\n",
    "# with open('../dataset/corpus.txt', 'w') as file:\n",
    "#     file.write(long_text)\n",
    "\n",
    "# Train SentencePiece model directly from Python list of sentences\n",
    "spm.SentencePieceTrainer.train(input='../dataset/corpus.txt', model_prefix='spm_full_text_model', vocab_size=10000, model_type='unigram')\n",
    "\n",
    "print(train.shape)\n",
    "print(train['caption'])\n",
    "train = train.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('spm_05_text_model.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
