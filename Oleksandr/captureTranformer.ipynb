{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyarrow matplotlib sentencepiece pandas\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from both Parquet files\n",
    "train_0 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_1 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_2 = pd.read_parquet('../dataset/0002.parquet')\n",
    "train_3 = pd.read_parquet('../dataset/0003.parquet')\n",
    "train_4 = pd.read_parquet('../dataset/0004.parquet')\n",
    "train_5 = pd.read_parquet('../dataset/0005.parquet')\n",
    "train_6 = pd.read_parquet('../dataset/0006.parquet')\n",
    "train_7 = pd.read_parquet('../dataset/0007.parquet')\n",
    "train_8 = pd.read_parquet('../dataset/0008.parquet')\n",
    "\n",
    "train = pd.concat([train_0, train_1, train_2, train_3, train_4, train_5, train_6, train_7, train_8,], ignore_index=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# Concatenate elements within each row\n",
    "train['caption'] = train['caption'].apply('\\n'.join)\n",
    "# Concatenate all rows into one long text string\n",
    "long_text = train['caption'].str.cat(sep='\\n')\n",
    "with open('../dataset/corpus.txt', 'w') as file:\n",
    "    file.write(long_text)\n",
    "\n",
    "# Train SentencePiece model directly from Python list of sentences\n",
    "spm.SentencePieceTrainer.train(input='../dataset/corpus.txt', model_prefix='../dataset/spm_10000_vocab_model', vocab_size=10000, model_type='unigram')\n",
    "\n",
    "print(train.shape)\n",
    "print(train['caption'])\n",
    "train = train.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../dataset/spm_10000_vocab_model.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
