{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyarrow matplotlib sentencepiece pandas\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31014, 6)\n",
      "0        Two young guys with shaggy hair look at their ...\n",
      "1        Several men in hard hats are operating a giant...\n",
      "2        A child in a pink dress is climbing up a set o...\n",
      "3        Someone in a blue shirt and hat is standing on...\n",
      "4        Two men, one in a gray shirt, one in a black s...\n",
      "                               ...                        \n",
      "31009    Woman writing on a pad in room with gold, deco...\n",
      "31010    A person in a red shirt climbing up a rock fac...\n",
      "31011    Two male construction workers are working on a...\n",
      "31012    An older busker in glasses plays an Eastern st...\n",
      "31013    A man in shorts and a Hawaiian shirt leans ove...\n",
      "Name: caption, Length: 31014, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../dataset/corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_full_text_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../dataset/corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 155070 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=9955402\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9528% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=53\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999528\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 155070 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=5635125\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 51364 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 155070\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 34068\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 34068 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=20880 obj=8.92546 num_tokens=68448 num_tokens/piece=3.27816\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16455 obj=6.78882 num_tokens=68315 num_tokens/piece=4.15163\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12339 obj=6.73823 num_tokens=72597 num_tokens/piece=5.88354\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12335 obj=6.72894 num_tokens=72599 num_tokens/piece=5.88561\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=6.73782 num_tokens=75125 num_tokens/piece=6.82955\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=6.73442 num_tokens=75130 num_tokens/piece=6.83\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_full_text_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_full_text_model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Read data from both Parquet files\n",
    "train_0 = pd.read_parquet('../dataset/0000.parquet')\n",
    "train_1 = pd.read_parquet('../dataset/0001.parquet')\n",
    "train_2 = pd.read_parquet('../dataset/0002.parquet')\n",
    "train_3 = pd.read_parquet('../dataset/0003.parquet')\n",
    "train_4 = pd.read_parquet('../dataset/0004.parquet')\n",
    "train_5 = pd.read_parquet('../dataset/0005.parquet')\n",
    "train_6 = pd.read_parquet('../dataset/0006.parquet')\n",
    "train_7 = pd.read_parquet('../dataset/0007.parquet')\n",
    "train_8 = pd.read_parquet('../dataset/0008.parquet')\n",
    "\n",
    "train = pd.concat([train_0, train_1, train_2, train_3, train_4, train_5, train_6, train_7, train_8,], ignore_index=True)\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "# Concatenate elements within each row\n",
    "train['caption'] = train['caption'].apply('\\n'.join)\n",
    "# Concatenate all rows into one long text string\n",
    "long_text = train['caption'].str.cat(sep='\\n')\n",
    "with open('../dataset/corpus.txt', 'w') as file:\n",
    "    file.write(long_text)\n",
    "\n",
    "# Train SentencePiece model directly from Python list of sentences\n",
    "spm.SentencePieceTrainer.train(input='../dataset/corpus.txt', model_prefix='spm_full_text_model', vocab_size=10000, model_type='unigram')\n",
    "\n",
    "print(train.shape)\n",
    "print(train['caption'])\n",
    "train = train.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('../dataset/spm_10000_vocab_model.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
